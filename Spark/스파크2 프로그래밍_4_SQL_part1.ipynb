{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "## Spark SQl 개요\n",
    "* RDD는 스키마를 표현할수 없다는 단점이 있음, 스파크SQL모듈은 이를 보완할 수 있는 데이터 모델(데이터셋)과 API를 제공.\n",
    "* RDD와 마찬가지로 액션과 트랜스포메이션으로 구성, 데이터셋의 트렌스포메이션 연산은 타입과 비타입 연산으로 나뉨  \n",
    "타입 연산(typed operations) : 기존과 동일한 타입으로 연산 (RDD)  \n",
    "비타입 연산(untyped opterations) : 기존과 다른 타입으로 연산 (int를 org.apache.spark.sql.Column 등) 로우와 칼럼 단위로 데이터를 처리함. \n",
    "* 데이터프레임은 org.apache.spark.sql.Row타입의 요소로 구성된 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.types import LongType, BooleanType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# from word import Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 작성 및 단어 수 세기 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 세션 생성\n",
    "spark = SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"Sample\")\\\n",
    "          .master(\"local[*]\")\\\n",
    "          .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 생성\n",
    "source = \"/usr/local/Cellar/apache-spark/2.3.2/README.md\"\n",
    "\n",
    "# 스파크 세션의 read() 메서드는 데이터를 읽기 위한 DataFrameReader 인스턴스를 돌려주는데 이 DataFrameReader를 이용해 다양한 유형의\n",
    "# 데이터로 부터 데이터 프레임을 생성할 수 있음. 아래의 예제는 text 파일이기 떄문에 text()를 사용.\n",
    "df = spark.read.text(source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|              online|    1|\n",
      "|              graphs|    1|\n",
      "|          [\"Parallel|    1|\n",
      "|          [\"Building|    1|\n",
      "|              thread|    1|\n",
      "|       documentation|    3|\n",
      "|            command,|    2|\n",
      "|         abbreviated|    1|\n",
      "|            overview|    1|\n",
      "|                rich|    1|\n",
      "|                 set|    2|\n",
      "|         -DskipTests|    1|\n",
      "|                name|    1|\n",
      "|page](http://spar...|    1|\n",
      "|        [\"Specifying|    1|\n",
      "|              stream|    1|\n",
      "|                run:|    1|\n",
      "|                 not|    1|\n",
      "|            programs|    2|\n",
      "|               tests|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 단어 수 카운트 코드\n",
    "# row와 column 단위로 데이터를 처리함 (비타입 연산)\n",
    "\n",
    "wordDF = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "# df는 스파크 세션의 read() 메서드를 이용해 생성한 데이터프레임.\n",
    "# 칼럼명을 따로 지정하지 않으면 \"value\"를 기본값으로 생성\n",
    "# 따라서 df클래스의 메서드 select을 사용해 column 선택(?)\n",
    "#      explode/split(= funtions 오브젝트의 메서드)를 각각 수행.\n",
    "#      .alias 는 column 클래스의 메서드\n",
    "\n",
    "result = wordDF.groupBy(\"word\").count()\n",
    "\n",
    "# \n",
    "result.show()\n",
    "# result.write().text(\"<path_to_save>\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스파크세션\n",
    "* 스파크세션은 데이터프레임(DataFrame) 또는 데이터셋(DataSet)을 생성하거나 사용자 정의함수(UDF) 등록 가능.\n",
    "* bulder 메서드를 이용해 생성 가능.\n",
    "* 과거엔 SQLContext와 HiveContext를 사용했으나 스파크2.0에선 합쳐짐. SQLContext + HiveContext = SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 세션 생성\n",
    "spark = SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"Sample\")\\\n",
    "          .master(\"local[*]\")\\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터프레임, 로우, 컬럼\n",
    "* DataFrame은 Dataset[Row]인 경우로 R의 데이터프레임이나 데이터베이스의 테이블과 비슷한 구조.\n",
    "\n",
    "### 데이터프레임 생성\n",
    "* 데이터프레임은 스파크세션을 이용해 생성, 외부파일이나 이미 생성된 RDD를 데이터프레임 형태로 생성 가능.  \n",
    "  \n",
    "    [외부 파일로 부터 데이터 생성]  \n",
    "    (1) 스파크세션의 read() 메서드를 호출해 DataFrameReader 인스턴스를 생성함.  \n",
    "    (2) format() 메서드로 데이터소스의 유형을 지정함.  \n",
    "    (3) option() 메서드로 데이터소스 처리에 필요한 옵션을 저장함.  \n",
    "    (4) load() 메서드로 대상 파일을 읽고 데이터프레임을 생성함.  \n",
    "  \n",
    "    [기존 RDD 및 로컬에서 데이터 생성]  \n",
    "    (1) RDD와 달리 스키마 정보를 함꼐 지정.( 리플렉션 API를 활용 or  직접 정보 작성)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리플렉션을 통한 데이터프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+\n",
      "|age|           job|  name|\n",
      "+---+--------------+------+\n",
      "|  7|       student|hayoon|\n",
      "| 13|       student|sunwoo|\n",
      "|  5|kindergartener| hajoo|\n",
      "| 13|       student|jinwoo|\n",
      "+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row1 = Row(name=\"hayoon\", age=7, job=\"student\")\n",
    "row2 = Row(name=\"sunwoo\", age=13, job=\"student\")\n",
    "row3 = Row(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "row4 = Row(name=\"jinwoo\", age=13, job=\"student\")\n",
    "data = [row1, row2, row3, row4]\n",
    "sample_df = spark.createDataFrame(data)\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기존 RDD를 통한 데이터프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.applySchemaToPythonRDD.\n: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:578)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:753)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:738)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\t... 30 more\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\t... 45 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\t... 51 more\nCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:578)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:753)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:738)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 113 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /Users/arnorfati/Data_Scientist/Spark2_programming/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 110 more\n------\r\n\nNestedThrowables:\njava.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:578)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:753)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:738)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 113 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /Users/arnorfati/Data_Scientist/Spark2_programming/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 110 more\n------\r\n\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\t... 56 more\nCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:578)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:753)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:738)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 113 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /Users/arnorfati/Data_Scientist/Spark2_programming/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 110 more\n------\r\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\t... 85 more\nCaused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\t... 97 more\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b705cb6, see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 113 more\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database /Users/arnorfati/Data_Scientist/Spark2_programming/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\t... 110 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-58b1edb5ea18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 기존 RDD를 dataframe으로 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample_df2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msample_df2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'"
     ]
    }
   ],
   "source": [
    "# 기존 RDD를 dataframe으로 생성\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "sample_df2 = spark.createDataFrame(data)\n",
    "sample_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 명시적 타입 지정을 통한 데이터프레임 생성\n",
    "* 스키마 정보를 입력해야 함.  \n",
    "StructField : 컬럼 정보  \n",
    "StructType : 로우 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+------+------+\n",
      "|   name|age|           job|salary|target|\n",
      "+-------+---+--------------+------+------+\n",
      "| hayoon|  7|       student|  2000|     1|\n",
      "| sunwoo| 13|       student|  3000|     0|\n",
      "|  hajoo|  5|kindergartener|  2000|     1|\n",
      "| jinwoo| 13|       student|  1000|     0|\n",
      "|hyukjoo| 27|            DS|  4000|     0|\n",
      "+-------+---+--------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 명시적 타입 지정을 통해 데이터프레임 생성 - 사용가능한 타입은 org.apache.spark.sql.types.DataType에서 참조 가능\n",
    "\n",
    "sf1 = StructField(\"name\", StringType(), True) # 컬럼에 대한 스키마 정보\n",
    "#                 컬럼이름,  타입         , null 허용 여부\n",
    "sf2 = StructField(\"age\", IntegerType(), True)\n",
    "sf3 = StructField(\"job\", StringType(), True)\n",
    "sf4 = StructField(\"salary\", IntegerType(), True)\n",
    "sf5 = StructField(\"target\", IntegerType(), True)\n",
    "schema = StructType([sf1, sf2, sf3, sf4, sf5]) # 로우에 대한 스키마 정보\n",
    "r1 = (\"hayoon\", 7, \"student\",2000,1)\n",
    "r2 = (\"sunwoo\", 13, \"student\",3000,0)\n",
    "r3 = (\"hajoo\", 5, \"kindergartener\",2000,1)\n",
    "r4 = (\"jinwoo\", 13, \"student\",1000,0)\n",
    "r5 = (\"hyukjoo\", 27, \"DS\",4000,0)\n",
    "rows = [r1, r2, r3, r4,r5]\n",
    "sample_df3 = spark.createDataFrame(rows, schema)\n",
    "sample_df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임 주요 연산 및 사용법\n",
    "* 데이터프레임은 org.apache.spark.sql.Row 타입의 객체로 구성된 데이터 셋 !!  \n",
    "* 데이터셋과 동일한 타입임에도 별도의 데이터프레임이라고 하는 이유는 사용 가능한 트랜스포메이션 연산의 종류가 다르기 때문.\n",
    "#### 액션 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+------+------+\n",
      "|   name|age|           job|salary|target|\n",
      "+-------+---+--------------+------+------+\n",
      "| hayoon|  7|       student|  2000|     1|\n",
      "| sunwoo| 13|       student|  3000|     0|\n",
      "|  hajoo|  5|kindergartener|  2000|     1|\n",
      "| jinwoo| 13|       student|  1000|     0|\n",
      "|hyukjoo| 27|            DS|  4000|     0|\n",
      "+-------+---+--------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show() - 데이터셋(프레임)에 저장된 데이터를 화면에 출력\n",
    "sample_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='hayoon', age=7, job='student', salary=2000, target=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head()/first() - 데이터셋의 첫 번째 로우를 돌려줌.\n",
    "sample_df3.head() # = sample_df3.first()\n",
    "# = take 함수와 동일\n",
    "sample_df3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count() - Row개수를 리턴해줌\n",
    "sample_df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='hayoon', age=7, job='student', salary=2000, target=1),\n",
       " Row(name='sunwoo', age=13, job='student', salary=3000, target=0),\n",
       " Row(name='hajoo', age=5, job='kindergartener', salary=2000, target=1),\n",
       " Row(name='jinwoo', age=13, job='student', salary=1000, target=0),\n",
       " Row(name='hyukjoo', age=27, job='DS', salary=4000, target=0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect()/ collectAsList() - 데이터셋의 모든 데이터를 로컬 컬렉션 형태로 돌려줌. !!메모리에 적재됨으로 메모리 부족 에러가 발생하지 않도록 주의!!\n",
    "sample_df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|              age|           salary|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|                5|                5|\n",
      "|   mean|             13.0|           2400.0|\n",
      "| stddev|8.602325267042627|1140.175425099138|\n",
      "|    min|                5|             1000|\n",
      "|    max|               27|             4000|\n",
      "+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "sample_df3.describe([\"age\",\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기본 연산\n",
    "* 데이터셋이 제공하는 연산은 크게 기본/ 타입 트랜스포메이션/ 비타입 트랜스포메이션/ 액션 연산으로 구성.  \n",
    "* 비타입 트랜스포메이션 연산은 데이터프레임인 경우만 사용 가능.  \n",
    "* 타입 트랜스포메이션 연산은 데이터셋인 경우만 사용 가능.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작업 중인 데이터를 메모리에 저장함.\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 정보를 조회.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'job', 'salary', 'target']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스키마 정보를 조회하는 다른 방법들.\n",
    "df.columns\n",
    "df.dtypes\n",
    "# df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 테이블 처럼 SQL을 사용해 처리할 수 있게 등록해 줌.\n",
    "sample_df3.createOrReplaceTempView(\"users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|hyukjoo| 27|\n",
      "+-------+---+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [name#21, age#22]\n",
      "+- *(1) Filter (isnotnull(age#22) && (age#22 > 20))\n",
      "   +- Scan ExistingRDD[name#21,age#22,job#23,salary#24,target#25]\n"
     ]
    }
   ],
   "source": [
    "# 쿼리를 날리는 예시\n",
    "\n",
    "spark.sql(\"select name, age from users where age > 20\").show()\n",
    "spark.sql(\"select name, age from users where age > 20\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비타입 트랜스포메이션 연산\n",
    "* 데이터의 실제 타입을 사용하지 않는 변환 연산을 수행.  \n",
    "예) SELECT * FROM PERSON WHERE AGE > 10과 같은 SQL문에서 AGE 칼럼은 정수값을 갖지만 타입을 정수라고 기재하지 않음. 스파크 2.0에서는 데이터 셋이 등장하면서 타입을 고려한 연산과 그렇지 않은 연산 2가지가 존재해 구분차 '비타입'이 생김."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.2.1.1절 ~ 5.5.2.2.4절\n",
    "def runBasicOpsEx(spark, sc, df):\n",
    "    df.show()\n",
    "    df.head()\n",
    "    df.first()\n",
    "    df.take(2)\n",
    "    df.count()\n",
    "    df.collect()\n",
    "    df.describe(\"age\").show()\n",
    "   \n",
    "\n",
    "# 5.5.2.4절\n",
    "def runColumnEx(spark, sc, df):\n",
    "    df.where(df.age > 10).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.2절\n",
    "def runAlias(spark, sc, df):\n",
    "    df.select(df.age + 1).show()\n",
    "    df.select((df.age + 1).alias(\"age\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.3절\n",
    "def runIsinEx(spark, sc):\n",
    "    nums = spark.sparkContext.broadcast([1, 3, 5, 7, 9])\n",
    "    rdd = spark.sparkContext.parallelize(range(0, 10)).map(lambda v: Row(v))\n",
    "    df = spark.createDataFrame(rdd)\n",
    "    df.where(df._1.isin(nums.value)).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.4절\n",
    "def runWhenEx(spark, sc):\n",
    "    ds = spark.range(0, 5)\n",
    "    col = when(ds.id % 2 == 0, \"even\").otherwise(\"odd\").alias(\"type\")\n",
    "    ds.select(ds.id, col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.5절\n",
    "def runMaxMin(spark, df):\n",
    "    min_col = min(\"age\")\n",
    "    max_col = max(\"age\")\n",
    "    df.select(min_col, max_col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.6절 ~ 5.5.2.4.9절\n",
    "def runAggregateFunctions(spark, df1, df2):\n",
    "    # collect_list, collect_set\n",
    "    doubledDf1 = df1.union(df1)\n",
    "    doubledDf1.select(functions.collect_list(doubledDf1[\"name\"])).show(truncate=False)\n",
    "    doubledDf1.select(functions.collect_set(doubledDf1[\"name\"])).show(truncate=False)\n",
    "\n",
    "    # count, countDistinct\n",
    "    doubledDf1.select(functions.count(doubledDf1[\"name\"]), functions.countDistinct(doubledDf1[\"name\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "    # sum\n",
    "    df2.printSchema()\n",
    "    df2.select(sum(df2[\"price\"])).show(truncate=False)\n",
    "\n",
    "    # grouping, grouping_id\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping(df2[\"store\"])).show(truncate=False)\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping_id(df2[\"store\"], df2[\"product\"])).show(\n",
    "        truncate=False)\n",
    "    \n",
    "    # grouping_id를 이용한 정렬\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]) \\\n",
    "        .agg(sum(\"amount\").alias(\"sum\"), grouping_id(\"store\", \"product\").alias(\"gid\")) \\\n",
    "        .filter(\"gid != '2'\") \\\n",
    "        .sort(asc(\"store\"), col(\"gid\")) \\\n",
    "        .na.fill({\"store\":\"Total\", \"product\":\"-\"}) \\\n",
    "        .select(\"store\", \"product\", \"sum\") \\\n",
    "        .show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.12 ~ 5.5.2.4.14절\n",
    "def runDateFunctions(spark):\n",
    "    f1 = StructField(\"d1\", StringType(), True)\n",
    "    f2 = StructField(\"d2\", StringType(), True)\n",
    "    schema1 = StructType([f1, f2])\n",
    "\n",
    "    df = spark.createDataFrame([(\"2017-12-25 12:00:05\", \"2017-12-25\")], schema1)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # current_date, unix_timestamp, to_date\n",
    "    d3 = current_date().alias(\"d3\")\n",
    "    d4 = unix_timestamp(df[\"d1\"].alias(\"d4\"))\n",
    "    d5 = to_date(df[\"d2\"].alias(\"d5\"))\n",
    "    d6 = to_date(d4.cast(\"timestamp\")).alias(\"d6\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d3, d4, d5, d6).show(truncate=False)\n",
    "\n",
    "    # add_months, date_add, last_day\n",
    "    d7 = add_months(d6, 2).alias(\"d7\")\n",
    "    d8 = date_add(d6, 2).alias(\"d8\")\n",
    "    d9 = last_day(d6).alias(\"d9\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d7, d8, d9).show(truncate=False)\n",
    "\n",
    "    # window\n",
    "    f3 = StructField(\"date\", StringType(), True)\n",
    "    f4 = StructField(\"product\", StringType(), True)\n",
    "    f5 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema2 = StructType([f3, f4, f5])\n",
    "\n",
    "    r2 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    r3 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    r4 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    r5 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    r6 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    r7 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    r8 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    r9 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([r2, r3, r4, r5, r6, r7, r8, r9], schema2);\n",
    "\n",
    "    timeCol = unix_timestamp(dd[\"date\"]).cast(\"timestamp\");\n",
    "    windowCol = window(timeCol, \"5 minutes\");\n",
    "    dd.groupBy(windowCol, dd[\"product\"]).agg(sum(dd[\"amount\"])).show(truncate=False);\n",
    "\n",
    "\n",
    "# 5.5.2.4.15절\n",
    "def runDateFunctions(spark):\n",
    "    # 파이썬의 경우 아래와 같이 튜플을 이용하여 데이터프레임을 생성하는 것도 가능함\n",
    "    df1 = spark.createDataFrame([(1.512,), (2.234,), (3.42,)], ['value'])\n",
    "    df2 = spark.createDataFrame([(25.0,), (9.0,), (10.0,)], ['value'])\n",
    "\n",
    "    df1.select(round(df1[\"value\"], 1)).show()\n",
    "    df2.select(functions.sqrt('value')).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.16 ~ 5.5.2.4.20절\n",
    "def runOtherFunctions(spark, personDf):\n",
    "    df = spark.createDataFrame([(\"v1\", \"v2\", \"v3\")], [\"c1\", \"c2\", \"c3\"]);\n",
    "\n",
    "    # array\n",
    "    df.select(df.c1, df.c2, df.c3, array(\"c1\", \"c2\", \"c3\").alias(\"newCol\")).show(truncate=False)\n",
    "\n",
    "    # desc, asc\n",
    "    personDf.show()\n",
    "    personDf.sort(functions.desc(\"age\"), functions.asc(\"name\")).show()\n",
    "\n",
    "    # pyspark 2.1.0 버전은 desc_nulls_first, desc_nulls_last, asc_nulls_first, asc_nulls_last 지원하지 않음\n",
    "\n",
    "    # split, length (pyspark에서 컬럼은 df[\"col\"] 또는 df.col 형태로 사용 가능)\n",
    "    df2 = spark.createDataFrame([(\"Splits str around pattern\",)], ['value'])\n",
    "    df2.select(df2.value, split(df2.value, \" \"), length(df2.value)).show(truncate=False)\n",
    "\n",
    "    # rownum, rank\n",
    "    f1 = StructField(\"date\", StringType(), True)\n",
    "    f2 = StructField(\"product\", StringType(), True)\n",
    "    f3 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema = StructType([f1, f2, f3])\n",
    "\n",
    "    p1 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    p2 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    p3 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    p4 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    p5 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    p6 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    p7 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    p8 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([p1, p2, p3, p4, p5, p6, p7, p8], schema)\n",
    "    w1 = Window.partitionBy(\"product\").orderBy(\"amount\")\n",
    "    w2 = Window.orderBy(\"amount\")\n",
    "    dd.select(dd.product, dd.amount, functions.row_number().over(w1).alias(\"rownum\"),\n",
    "              functions.rank().over(w2).alias(\"rank\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.21절\n",
    "def runUDF(spark, df):\n",
    "    # functions를 이용한 등록\n",
    "    fn1 = functions.udf(lambda job: job == \"student\")\n",
    "    df.select(df[\"name\"], df[\"age\"], df[\"job\"], fn1(df[\"job\"])).show()\n",
    "    # SparkSession을 이용한 등록\n",
    "    spark.udf.register(\"fn2\", lambda job: job == \"student\")\n",
    "    df.createOrReplaceTempView(\"persons\")\n",
    "    spark.sql(\"select name, age, job, fn2(job) from persons\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.24절\n",
    "def runAgg(spark, df):\n",
    "    df.agg(max(\"amount\"), min(\"price\")).show()\n",
    "    df.agg({\"amount\": \"max\", \"price\": \"min\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.26절\n",
    "def runDfAlias(spark, df):\n",
    "    df.select(df[\"product\"]).show()\n",
    "    df.alias(\"aa\").select(\"aa.product\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.27절\n",
    "def runGroupBy(spark, df):\n",
    "    df.groupBy(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.3.4.28절\n",
    "def runCube(spark, df):\n",
    "    df.cube(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.29절\n",
    "def runDistinct(spark):\n",
    "    d1 = (\"store1\", \"note\", 20, 2000)\n",
    "    d2 = (\"store1\", \"bag\", 10, 5000)\n",
    "    d3 = (\"store1\", \"note\", 20, 2000)\n",
    "    rows = [d1, d2, d3]\n",
    "    cols = [\"store\", \"product\", \"amount\", \"price\"]\n",
    "    df = spark.createDataFrame(rows, cols)\n",
    "    df.distinct().show()\n",
    "    df.dropDuplicates([\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.30절\n",
    "def runDrop(spark, df):\n",
    "    df.drop(df[\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.31절\n",
    "def runIntersect(spark):\n",
    "    a = spark.range(1, 5)\n",
    "    b = spark.range(2, 6)\n",
    "    c = a.intersect(b)\n",
    "    c.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.32절\n",
    "def runExcept(spark):\n",
    "    df1 = spark.range(1, 6)\n",
    "    df2 = spark.createDataFrame([(2,), (4,)], ['value'])\n",
    "    # 파이썬의 경우 except 대신 subtract 메서드 사용\n",
    "    # subtract의 동작은 except와 같음\n",
    "    df1.subtract(df2).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.33절\n",
    "def runJoin(spark, ldf, rdf):\n",
    "    joinTypes = \"inner,outer,leftouter,rightouter,leftsemi\".split(\",\")\n",
    "    for joinType in joinTypes:\n",
    "        print(\"============= %s ===============\" % joinType)\n",
    "        ldf.join(rdf, [\"word\"], joinType).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.35절\n",
    "def runNa(spark, ldf, rdf):\n",
    "    result = ldf.join(rdf, [\"word\"], \"outer\").toDF(\"word\", \"c1\", \"c2\")\n",
    "    result.show()\n",
    "    # 파이썬의 경우 na.drop또는 dropna 사용 가능\n",
    "    # c1과 c2 칼럼의 null이 아닌 값의 개수가 thresh 이하일 경우 drop\n",
    "    # thresh=1로 설정할 경우 c1 또는 c2 둘 중의 하나만 null 아닌 값을 가질 경우\n",
    "    # 결과에 포함시킨다는 의미가 됨\n",
    "    result.na.drop(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    result.dropna(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    # fill\n",
    "    result.na.fill({\"c1\": 0}).show()\n",
    "    # 파이썬의 경우 to_replace에 딕셔너리를 지정하여 replace를 수행(이 경우 value에 선언한 값은 무시됨\n",
    "    # 딕셔너리를 사용하지 않을 경우 키 목록(첫번째 인자)과 값 목록(두번째 인자)을 지정하여 replace 수행\n",
    "    result.na.replace(to_replace={\"w1\": \"word1\", \"w2\": \"word2\"}, value=\"\", subset=\"word\").show()\n",
    "    result.na.replace([\"w1\", \"w2\"], [\"word1\", \"word2\"], \"word\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.36절\n",
    "def runOrderBy(spark):\n",
    "    df = spark.createDataFrame([(3, \"z\"), (10, \"a\"), (5, \"c\")], [\"idx\", \"name\"])\n",
    "    df.orderBy(\"name\", \"idx\").show()\n",
    "    df.orderBy(\"idx\", \"name\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.37절\n",
    "def runRollup(spark, df):\n",
    "    df.rollup(\"store\", \"product\").agg({\"price\": \"sum\"}).show();\n",
    "\n",
    "\n",
    "# 5.5.2.4.38절\n",
    "def runStat(spark):\n",
    "    df = spark.createDataFrame([(\"a\", 6), (\"b\", 4), (\"c\", 12), (\"d\", 6)], [\"word\", \"count\"])\n",
    "    df.show()\n",
    "    df.stat.crosstab(\"word\", \"count\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.39절\n",
    "def runWithColumn(spark):\n",
    "    df1 = spark.createDataFrame([(\"prod1\", \"100\"), (\"prod2\", \"200\")], [\"pname\", \"price\"])\n",
    "    df2 = df1.withColumn(\"dcprice\", df1[\"price\"] * 0.9)\n",
    "    df3 = df2.withColumnRenamed(\"dcprice\", \"newprice\")\n",
    "    df1.show()\n",
    "    df2.show()\n",
    "    df3.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.40절\n",
    "def runSave(spark):\n",
    "    sparkHomeDir = \"file:///Users/beginspark/Apps/spark\"\n",
    "    df = spark.read.json(sparkHomeDir + \"/examples/src/main/resources/people.json\")\n",
    "    df.write.save(\"/Users/beginspark/Temp/default/%d\" % time.time())\n",
    "    df.write.format(\"json\").save(\"/Users/beginspark/Temp/json/%d\" % time.time())\n",
    "    df.write.format(\"json\").partitionBy(\"age\").save(\"/Users/beginspark/Temp/parti/%d\" % time.time())\n",
    "    # saveMode: append, overwrite, error, ignore\n",
    "    df.write.mode(\"overwrite\").saveAsTable(\"ohMyTable\")\n",
    "    spark.sql(\"select * from ohMyTable\").show()\n",
    "    df.write.format('parquet').bucketBy(20, 'age').mode(\"overwrite\").saveAsTable(\"bucketTable\")\n",
    "    spark.sql(\"select * from bucketTable\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋\n",
    "* 기존의 RDD와 데이터프레임을 모두 활용하기 위해서 생김.\n",
    "* Python에서는 현재 미지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"sample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///Users/beginspark/Temp/\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 파이썬에서 데이터프레임 생성 시 네임드튜플(namedtuple), 튜플(tuple)\n",
    "# Row, 커스텀 클래스(class), 딕셔너리(dictionary) 등을\n",
    "# 사용하여 생성할 수 있다\n",
    "Person = collections.namedtuple('Person', 'name age job')\n",
    "\n",
    "# sample dataframe 1\n",
    "row1 = Person(name=\"hayoon\", age=7, job=\"student\")\n",
    "row2 = Person(name=\"sunwoo\", age=13, job=\"student\")\n",
    "row3 = Person(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "row4 = Person(name=\"jinwoo\", age=13, job=\"student\")\n",
    "data = [row1, row2, row3, row4]\n",
    "sample_df = spark.createDataFrame(data)\n",
    "\n",
    "d1 = (\"store2\", \"note\", 20, 2000)\n",
    "d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "d3 = (\"store1\", \"note\", 15, 1000)\n",
    "d4 = (\"store1\", \"pen\", 20, 5000)\n",
    "sample_df2 = spark.createDataFrame([d1, d2, d3, d4]).toDF(\"store\", \"product\", \"amount\", \"price\")\n",
    "\n",
    "ldf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w2\", 1)])\n",
    "rdf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w3\", 1)])\n",
    "\n",
    "\n",
    "# createDataFrame\n",
    "def createDataFrame(spark, sc):\n",
    "    sparkHomeDir = \"file:/Users/beginspark/Apps/spark\"\n",
    "\n",
    "    # 1. 외부 데이터소스로부터 데이터프레임 생성\n",
    "    df1 = spark.read.json(sparkHomeDir + \"/examples/src/main/resources/people.json\")\n",
    "    df2 = spark.read.parquet(sparkHomeDir + \"/examples/src/main/resources/users.parquet\")\n",
    "    df3 = spark.read.text(sparkHomeDir + \"/examples/src/main/resources/people.txt\")\n",
    "\n",
    "    # 2. 로컬 컬렉션으로부터 데이터프레임 생성 (ex5-17)\n",
    "    row1 = Row(name=\"hayoon\", age=7, job=\"student\")\n",
    "    row2 = Row(name=\"sunwoo\", age=13, job=\"student\")\n",
    "    row3 = Row(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "    row4 = Row(name=\"jinwoo\", age=13, job=\"student\")\n",
    "    data = [row1, row2, row3, row4]\n",
    "    df4 = spark.createDataFrame(data)\n",
    "\n",
    "    # 3. 기존 RDD로부터 데이터프레임 생성 (ex5-20)\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    df5 = spark.createDataFrame(data)\n",
    "\n",
    "    # 4. 스키마 지정을 통한 데이터프레임 생성(ex5-23)\n",
    "    sf1 = StructField(\"name\", StringType(), True)\n",
    "    sf2 = StructField(\"age\", IntegerType(), True)\n",
    "    sf3 = StructField(\"job\", StringType(), True)\n",
    "    schema = StructType([sf1, sf2, sf3])\n",
    "    r1 = (\"hayoon\", 7, \"student\")\n",
    "    r2 = (\"sunwoo\", 13, \"student\")\n",
    "    r3 = (\"hajoo\", 5, \"kindergartener\")\n",
    "    r4 = (\"jinwoo\", 13, \"student\")\n",
    "    rows = [r1, r2, r3, r4]\n",
    "    df6 = spark.createDataFrame(rows, schema)\n",
    "    \n",
    "    # 5. 이미지를 이용한 데이터프레임 생성\n",
    "    path = sparkHomeDir + \"/data/mllib/images\"\n",
    "    recursive = True\n",
    "    numPartitions = 2\n",
    "    dropImageFailures = True\n",
    "    sampleRatio = 1.0\n",
    "    seed = 0    \n",
    "    imgdf = ImageSchema.readImages(path, recursive, numPartitions, dropImageFailures, sampleRatio, seed)\n",
    "    \n",
    "    imgdf = imgdf.select(imgdf[\"image.origin\"], imgdf[\"image.height\"], imgdf[\"image.width\"], imgdf[\"image.nChannels\"], imgdf[\"image.mode\"])\n",
    "    # imgdf.printSchema()\n",
    "    # imgdf.show(10, False)    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# 5.5.2.4.41.2절 \n",
    "def run_conversion_with_arrow(spark):\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    sales_data = {'name': ['store2', 'store2', 'store1', 'store1'],\n",
    "                  'product': ['note', 'bag', 'note', 'pen'],\n",
    "                  'amount': [20, 10, 15, 20],\n",
    "                  'price': [2000, 5000, 1000, 5000]}\n",
    "    pdf = pd.DataFrame(sales_data)\n",
    "    print(pdf)\n",
    "    \n",
    "    # pandas dataframe -> spark dataframe\n",
    "    df = spark.createDataFrame(pdf).groupBy(\"name\").count()\n",
    "    df.show(10, False)\n",
    "    # spark dataframe -> pandas dataframe    \n",
    "    pdf2 = df.toPandas()\n",
    "    print(pdf2)\n",
    "\n",
    "    \n",
    "# 5.5.2.4.41.3절\n",
    "def run_pandas_scala_udf(spark):\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    # pandas udf 정의 \n",
    "    total_price = pandas_udf(get_total_price, returnType=LongType())\n",
    "    # spark dataframe\n",
    "    sample_df2.show()\n",
    "    # padas 함수 적용 \n",
    "    sample_df2.withColumn(\"total_price\", total_price(col(\"amount\"), col(\"price\"))).show()\n",
    "\n",
    "    \n",
    "def get_total_price(amount, price):\n",
    "    return amount * price\n",
    "\n",
    "\n",
    "# 5.5.2.4.43절\n",
    "def run_pandas_grouped_map_udf(spark):\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    # spark dataframe\n",
    "    sample_df2.show()\n",
    "    # padas 함수 적용 \n",
    "    sample_df2.groupby(\"product\").apply(get_total_price_bydf).show()\n",
    "\n",
    "    \n",
    "@pandas_udf(\"store string, product string, amount int, price int, total_price int\", PandasUDFType.GROUPED_MAP)\n",
    "def get_total_price_bydf(pdf):\n",
    "    amount = pdf.amount\n",
    "    pdf['totol_price'] = (pdf.amount * pdf.price)\n",
    "    return pdf    \n",
    "\n",
    "\n",
    "# 실행은 bin/pyspark --jars <path>/beginning-spark-examples.jar와 같이 실행 \n",
    "def run_java_udf(pdf, df):\n",
    "    # python udf\n",
    "    spark.udf.register(\"fn2\", lambda job: job == \"student\")\n",
    "    df.createOrReplaceTempView(\"persons\")\n",
    "    spark.sql(\"select name, age, job, fn2(job) from persons\").show()\n",
    "    # functions를 이용한 등록\n",
    "    spark.udf.registerJavaFunction(\"judf\", \"com.wikibooks.spark.ch5.Judf\", BooleanType())\n",
    "    spark.sql(\"select name, age, job, judf(job) from persons\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.41.4절(파이썬의 경우 partitionByRange 는 지원하지 않음(spark2.3.0))\n",
    "def runPartitionByRange(spark):\n",
    "    pass \n",
    "\n",
    "\n",
    "# 5.5.2.4.41.5절\n",
    "def runColRegex(spark):\n",
    "    d1 = (\"store2\", \"note\", 20, 2000)\n",
    "    d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "    df = spark.createDataFrame([d1, d2]).toDF(\"store_nm\", \"prod_nm\", \"amount\", \"price\")\n",
    "    df.select(df.colRegex(\"`.*nm`\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.41.6절    \n",
    "def runUnionByName(spark):\n",
    "    d1 = (\"store2\", \"note\", 20, 2000)\n",
    "    d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "    df1 = spark.createDataFrame([d1, d2]).toDF(\"store_nm\", \"prod_nm\", \"amount\", \"price\")\n",
    "    df2 = df1.select(\"price\", \"amount\", \"prod_nm\", \"store_nm\")\n",
    "    df1.union(df2).show()\n",
    "    df1.unionByName(df2).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.41.7절\n",
    "def runToJson(spark):\n",
    "    d1 = (\"store2\", \"note\", 20, 2000)\n",
    "    d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "    df = spark.createDataFrame([d1, d2]).toDF(\"store_nm\", \"prod_nm\", \"amount\", \"price\")\n",
    "    df.select(to_json(struct(\"store_nm\", \"prod_nm\", \"amount\", \"price\")).alias(\"value\")).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.41.7절\n",
    "def runFromJson(spark):\n",
    "    v1 = (\"\"\"{\"store_nm\":\"store2\", \"prod_nm\":\"note\", \"amount\":20, \"price\":2000}\"\"\",)\n",
    "    v2 = (\"\"\"{\"store_nm\":\"store2\", \"prod_nm\":\"bag\",\"amount\":10, \"price\":5000}\"\"\",)\n",
    "\n",
    "    f1 = StructField(\"store_nm\", StringType(), True)\n",
    "    f2 = StructField(\"prod_nm\", StringType(), True)\n",
    "    f3 = StructField(\"amount\", IntegerType(), True)\n",
    "    f4 = StructField(\"price\", IntegerType(), True)\n",
    "    schema = StructType([f1, f2, f3, f4])\n",
    "    \n",
    "    options = {\"multiLine\" : \"false\"}\n",
    "    df1 = spark.createDataFrame([v1, v2]).toDF(\"value\")\n",
    "    df2 = df1.select(from_json(df1.value, schema, options).alias(\"value\"))\n",
    "    df3 = df2.select(df2['value']['store_nm'], df2['value']['prod_nm'], df2['value']['amount'], df2['value']['price'])\n",
    "    df3.toDF('store_nm', 'prod_nm', 'amount', 'price').show(truncate=False)\n",
    "\n",
    "    \n",
    "# [예제 실행 방법] 아래에서 원하는 예제의 주석을 제거하고 실행!!\n",
    "# createDataFrame(spark, sc)\n",
    "# runBasicOpsEx(spark, sc, sample_df)\n",
    "# runColumnEx(spark, sc, sample_df)\n",
    "# runAlias(spark, sc, sample_df)\n",
    "# runIsinEx(spark, sc)\n",
    "# runWhenEx(spark, sc)\n",
    "# runMaxMin(spark, sample_df)\n",
    "# runAggregateFunctions(spark, sample_df, sample_df2)\n",
    "# runCollectionFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runOtherFunctions(spark, sample_df)\n",
    "# runUDF(spark, sample_df)\n",
    "# runAgg(spark, sample_df2)\n",
    "# runDfAlias(spark, sample_df2)\n",
    "# runGroupBy(spark, sample_df2)\n",
    "# runCube(spark, sample_df2)\n",
    "# runDistinct(spark)\n",
    "# runDrop(spark, sample_df2)\n",
    "# runIntersect(spark)\n",
    "# runExcept(spark)\n",
    "# runJoin(spark, ldf, rdf)\n",
    "# runNa(spark, ldf, rdf)\n",
    "# runOrderBy(spark)\n",
    "# runRollup(spark, sample_df2)\n",
    "# runWithColumn(spark)\n",
    "# runSave(spark)\n",
    "# run_conversion_with_arrow(spark)\n",
    "# run_pandas_scala_udf(spark)\n",
    "# run_pandas_grouped_map_udf(spark)\n",
    "# runPartitionByRange(spark)\n",
    "# runColRegex(spark)\n",
    "# runUnionByName(spark)\n",
    "# runToJson(spark)\n",
    "# runFromJson(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAggregateFunctions(spark, df1, df2):\n",
    "    # collect_list, collect_set\n",
    "    doubledDf1 = df1.union(df1)\n",
    "    doubledDf1.select(functions.collect_list(doubledDf1[\"name\"])).show(truncate=False)\n",
    "    doubledDf1.select(functions.collect_set(doubledDf1[\"name\"])).show(truncate=False)\n",
    "\n",
    "    # count, countDistinct\n",
    "    doubledDf1.select(functions.count(doubledDf1[\"name\"]), functions.countDistinct(doubledDf1[\"name\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "    # sum\n",
    "    df2.printSchema()\n",
    "    df2.select(sum(df2[\"price\"])).show(truncate=False)\n",
    "\n",
    "    # grouping, grouping_id\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping(df2[\"store\"])).show(truncate=False)\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping_id(df2[\"store\"], df2[\"product\"])).show(\n",
    "        truncate=False)\n",
    "    \n",
    "    # grouping_id를 이용한 정렬\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]) \\\n",
    "        .agg(sum(\"amount\").alias(\"sum\"), grouping_id(\"store\", \"product\").alias(\"gid\")) \\\n",
    "        .filter(\"gid != '2'\") \\\n",
    "        .sort(asc(\"store\"), col(\"gid\")) \\\n",
    "        .na.fill({\"store\":\"Total\", \"product\":\"-\"}) \\\n",
    "        .select(\"store\", \"product\", \"sum\") \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
